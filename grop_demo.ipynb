{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOHWYPWuVVd/gt/ts6rsPHy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/paulkroe/minireason/blob/main/grop_demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import copy\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "def reward_function(prompt, completion):\n",
        "    # Simple reward: count unique characters in the completion.\n",
        "    return len(set(completion))\n",
        "\n",
        "def generate_completions(model, tokenizer, prompt, num_generations=3, max_length=50, temperature=1.0):\n",
        "    input_ids = tokenizer(prompt, return_tensors='pt').input_ids\n",
        "    outputs = model.generate(\n",
        "        input_ids=input_ids,\n",
        "        max_length=max_length,\n",
        "        do_sample=True,\n",
        "        num_return_sequences=num_generations,\n",
        "        temperature=temperature,\n",
        "    )\n",
        "    return [tokenizer.decode(output, skip_special_tokens=True) for output in outputs]\n",
        "\n",
        "def compute_log_probs(model, input_ids, attention_mask):\n",
        "    \"\"\"\n",
        "    Returns per-token log probabilities.\n",
        "    Shape: [batch_size, seq_len]\n",
        "    \"\"\"\n",
        "    outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "    logits = outputs.logits  # [B, L, V]\n",
        "    log_probs = torch.log_softmax(logits, dim=-1)\n",
        "    # Gather the log probabilities corresponding to the input_ids.\n",
        "    token_log_probs = torch.gather(log_probs, 2, input_ids.unsqueeze(-1)).squeeze(-1)\n",
        "    return token_log_probs\n",
        "\n",
        "def train_step(\n",
        "    model,\n",
        "    ref_model,\n",
        "    tokenizer,\n",
        "    optimizer,\n",
        "    prompt,\n",
        "    num_generations=3,\n",
        "    epsilon=0.2,\n",
        "    beta=0.1\n",
        "):\n",
        "    \"\"\"\n",
        "    A single training step implementing a PPO-like update on a per-token basis.\n",
        "    The KL divergence is computed per token using:\n",
        "        KL = exp(ref_logp - cur_logp) - (ref_logp - cur_logp) - 1\n",
        "    and the PPO objective is computed per token before averaging.\n",
        "    \"\"\"\n",
        "    # 1. Generate completions.\n",
        "    completions = generate_completions(model, tokenizer, prompt, num_generations=num_generations)\n",
        "\n",
        "    # 2. Compute rewards & advantages (one reward per generated completion).\n",
        "    rewards = [reward_function(prompt, comp) for comp in completions]\n",
        "    rewards_tensor = torch.tensor(rewards, dtype=torch.float32)\n",
        "    mean_reward = rewards_tensor.mean()\n",
        "    std_reward = rewards_tensor.std() + 1e-8  # avoid division by zero\n",
        "    advantages = (rewards_tensor - mean_reward) / std_reward  # one scalar advantage per generation\n",
        "\n",
        "    # Tokenize prompt to get its length.\n",
        "    prompt_ids = tokenizer(prompt, return_tensors='pt').input_ids\n",
        "    prompt_length = prompt_ids.shape[1]\n",
        "\n",
        "    total_loss = 0.0\n",
        "    for i, comp in enumerate(completions):\n",
        "        # 3. Prepare the full input (prompt + completion).\n",
        "        full_text = prompt + comp[len(prompt):]  # simple concatenation\n",
        "        full_ids = tokenizer(full_text, return_tensors='pt').input_ids\n",
        "        attention_mask = torch.ones_like(full_ids)  # assume no padding for simplicity\n",
        "\n",
        "        # 4. Get per-token log probabilities for the completion part.\n",
        "        cur_log_probs = compute_log_probs(model, full_ids, attention_mask)[0, prompt_length:]  # shape [T]\n",
        "        # Use the reference model to get the old log probabilities.\n",
        "        with torch.no_grad():\n",
        "            ref_log_probs = compute_log_probs(ref_model, full_ids, attention_mask)[0, prompt_length:]\n",
        "\n",
        "        # 5. Compute per-token probability ratios and clip them.\n",
        "        ratio = torch.exp(cur_log_probs - ref_log_probs)  # shape [T]\n",
        "        clipped_ratio = torch.clamp(ratio, 1 - epsilon, 1 + epsilon)\n",
        "\n",
        "        # The advantage is a single scalar per generation; broadcast it to each token.\n",
        "        adv = advantages[i]\n",
        "        per_token_loss1 = ratio * adv\n",
        "        per_token_loss2 = clipped_ratio * adv\n",
        "        # Per-token PPO loss (apply min for each token).\n",
        "        ppo_loss = -torch.min(per_token_loss1, per_token_loss2)\n",
        "\n",
        "        # 6. Compute per-token KL divergence.\n",
        "        per_token_kl = torch.exp(ref_log_probs - cur_log_probs) - (ref_log_probs - cur_log_probs) - 1\n",
        "\n",
        "        # 7. Combine the PPO loss and the weighted KL term (per token).\n",
        "        token_loss = ppo_loss + beta * per_token_kl\n",
        "\n",
        "        # 8. For simplicity, assume all tokens are valid (mask of ones).\n",
        "        completion_mask = torch.ones_like(cur_log_probs)\n",
        "        loss_i = (token_loss * completion_mask).sum() / completion_mask.sum()\n",
        "        total_loss += loss_i\n",
        "\n",
        "    # 9. Average loss over generations.\n",
        "    total_loss /= num_generations\n",
        "\n",
        "    # 10. Backpropagation and parameter update.\n",
        "    optimizer.zero_grad()\n",
        "    total_loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    print(f\"Prompt: {prompt}\")\n",
        "    print(f\"Completions: {completions}\")\n",
        "    print(f\"Rewards: {rewards}\")\n",
        "    print(f\"Advantages: {advantages.tolist()}\")\n",
        "    print(f\"Loss: {total_loss.item()}\")\n",
        "    return total_loss.item()\n",
        "\n",
        "def main():\n",
        "    # Load a small model & tokenizer (e.g. GPT-2).\n",
        "    model_name = \"gpt2\"\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "    model.train()\n",
        "\n",
        "    # Reference model (could be updated less frequently; here we update every step for simplicity).\n",
        "    ref_model = copy.deepcopy(model)\n",
        "    ref_model.eval()\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "    prompt = \"Once upon a time\"\n",
        "\n",
        "    # Run several training steps.\n",
        "    for step in range(3):\n",
        "        print(f\"\\n=== Training Step {step} ===\")\n",
        "        train_step(model, ref_model, tokenizer, optimizer, prompt, num_generations=3, epsilon=0.2, beta=0.1)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "iySV61mnMzbB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load a small model & tokenizer (e.g. GPT-2).\n",
        "model_name = \"gpt2\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "model.train()\n",
        "\n",
        "# Reference model (could be updated less frequently; here we update every step for simplicity).\n",
        "ref_model = copy.deepcopy(model)\n",
        "ref_model.eval()\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "prompt = \"Once upon a time\"\n",
        "\n",
        "# Run several training steps.\n",
        "for step in range(3):\n",
        "    print(f\"\\n=== Training Step {step} ===\")\n",
        "    train_step(model, ref_model, tokenizer, optimizer, prompt, num_generations=3, epsilon=0.2, beta=0.1)"
      ],
      "metadata": {
        "id": "Y3wDkAnXjUl5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}